why our statistical methods are better than original methods in the paper

[MODE: RESEARCH]

好的，這是一個非常關鍵的步驟。僅僅提出新方法是不夠的，我們必須能夠清晰地闡述，為什麼這些更複雜的統計方法在理論上和實踐上，都有潛力比論文中原始的方法做得更好。這一步是連接「創新」與「實證」之間的橋樑。

我將逐一分析我們提出的五種新方法，並將它們與論文中功能最接近的原始方法進行一對一的比較，以突顯其核心優勢。

---

### **1. 動差指標 (偏度/峰度) vs. `$GOP_{VarLogit}$` (變異數)**

* **原始方法 (`$GOP_{VarLogit}$`) 的核心**：只使用了分佈的二階動差（變異數），衡量 logit 數據點的「離散程度」。
* **我們的方法 (偏度/峰度) 的核心**：使用了三階和四階動差，衡量 logit 分佈的「不對稱性」和「尾部厚度/尖銳度」。

* **為何更好？**
    `$GOP_{VarLogit}$` 的根本缺陷在於，**同樣的變異數可以對應形狀完全不同的數據分佈**。它無法區分這些細微但關鍵的形狀差異。
    * **偏度 (Skewness)** 可以告訴我們發音信心的**演變趨勢**。例如，一個清晰的發音，其 logit 值可能是從低到高再平穩過渡（正偏），而一個猶豫或突然中斷的發音，其 logit 分佈可能是另一種傾斜方式。變異數完全無法捕捉這種方向性。
    * **峰度 (Kurtosis)** 可以告訴我們發音信心的**極端值分佈**。一個高峰度的分佈意味著大部分 logit 值都緊密圍繞在均值附近，只有少數極端值，這可能代表一個非常「典型」且穩定的發音。而一個低峰度的分佈（肥尾）則意味著出現極端高或極端低 logit 值的機率更高，這可能暗示著發音過程中的不穩定性或異常。

    **比喻**：`$GOP_{VarLogit}` 就像只知道一個班級學生成績的「標準差」，而我們的指標則像是同時知道了成績單是否「頭重腳輕」（偏度）以及是否存在大量的「學霸和學渣」（峰度）。後者顯然提供了更完整的畫面。

### **2. 資訊熵/散度 vs. `$GOP_{DNN}$`**

* **原始方法 (`$GOP_{DNN}$`) 的核心**：只關注目標音素的平均機率，忽略了所有其他競爭音素。
* **我們的方法 (資訊熵/散度) 的核心**：考量**整個**輸出機率分佈，衡量模型的「不確定性」或「混淆程度」。

* **為何更好？**
    `$GOP_{DNN}$` 可能會產生嚴重的誤導。想像一個場景，模型對目標音素 `/p/` 給出了 40% 的機率，這看起來還不錯。但同時，模型對 `/b/` 和 `/t/` 也分別給出了 30% 的高機率。在這種情況下：
    * `$GOP_{DNN}$` 的分數會相對較好，因為 40% 不算低。
    * 而**資訊熵**會非常高，因為整個分佈非常平坦，沒有一個明確的贏家。高熵直接揭示了模型的「極度困惑」，這是一個強烈的信號，表明發音很可能含糊不清，導致模型在多個選項間難以抉擇。
    
    `$GOP_{DNN}` 回答的是「模型認為這個音素**是**目標音素的機率有多大？」，而熵回答的是「模型對自己的判斷**有多麼確定**？」。在發音評估中，後者是一個遠比前者更魯棒、更深刻的問題。

### **3. 高斯混合模型 (GMM) vs. `$GOP_{VarLogit}$`**

* **原始方法 (`$GOP_{VarLogit}$`) 的核心**：隱含地假設 logit 分佈是一個單一的、簡單的分佈（單峰）。
* **我們的方法 (GMM) 的核心**：能夠對複雜的、**多峰 (multi-modal)** 的 logit 分佈進行建模。

* **為何更好？**
    音素的發音是一個動態過程，尤其像雙元音 `/aɪ/`（如 "eye"），它有明確的起始、滑動和結束階段。其對應的 logit 序列很可能不是一個簡單的單峰分佈，而是呈現出多個峰值或平台期。
    * `$GOP_{VarLogit}` 會將這些複雜的結構特徵全部「平均掉」，最終只得到一個籠統的「離散度」數字，丟失了所有內部結構資訊。
    * 而 **GMM** 則可以識別出這種多階段的結構。例如，一個擬合良好的雙峰 GMM 可以告訴我們，這個雙元音是否清晰地展現了兩個目標元音的階段，兩個階段的權重比例是否正常，以及它們之間的過渡是否平滑。這提供了對發音**內部動態過程**的洞察力，是簡單統計量無法比擬的。

### **4. 時間序列分析 vs. `$GOP_{VarLogit}$`**

* **原始方法 (`$GOP_{VarLogit}$`) 的核心**：將 logit 序列視為一個無序的「數字集合」，完全忽略了它們的**時間順序**。
* **我們的方法 (時間序列分析) 的核心**：明確地將 logit 序列作為一個時間序列來分析其**動態演變模式**。

* **為何更好？**
    兩個 logit 序列可以擁有完全相同的均值和變異數，但其時間模式可能天差地別。
    * 序列 A：`{5, 6, 7, 8, 7, 6, 5}` (平滑、對稱的變化)
    * 序列 B：`{8, 5, 7, 5, 6, 7, 6}` (劇烈、無規律的抖動)
    
    `$GOP_{VarLogit}` 無法區分 A 和 B。但顯然，序列 A 更像是一個穩定、流暢的發音過程，而序列 B 則暗示著發音的猶豫或顫抖。
    * **自相關函數 (Autocorrelation)** 能夠捕捉到序列 B 中的那種「抖動」（即時間上相鄰的值缺乏關聯性）。
    * **頻譜分析 (Spectral Analysis)** 則能從頻域上告訴我們這種變化的快慢。
    
    透過引入**時間維度**，我們能夠分析發音的「平滑度」和「節律」，這些都是評估發音自然度的關鍵指標。

### **5. 極值理論 (EVT) vs. `$GOP_{MaxLogit}$`**

* **原始方法 (`$GOP_{MaxLogit}$`) 的核心**：只依賴於單一個、最極端的數據點（最大值）。
* **我們的方法 (EVT) 的核心**：對**極端值的整體分佈**進行更穩健的統計建模。

* **為何更好？**
    `$GOP_{MaxLogit}` 的問題在於它非常「脆弱」。一個偶然的雜訊尖峰就可能導致它給出一個虛高的分數，反之亦然。它的可靠性完全取決於單一數據點的穩定性，這在充滿變數的語音訊號中是極其冒險的。
    * **Top-k 平均**：透過取前 k 個最大值的平均，我們能極大地**平滑掉單一異常值的影響**，得到一個更穩定的「峰值信心」估計。
    * **廣義極值分佈 (GEV)**：更進一步，GEV 擬合不僅僅是平均，它能分析極端值分佈的**形狀**，告訴我們出現極高 logit 值的「規律」。
    
    EVT 將問題從「有史以來最高的信心是多少？」轉變為一個更科學、更魯棒的問題：「**當模型處於高信心狀態時，其典型的、可重複的表現是怎樣的？**」。這顯然更能抵抗雜訊干擾，從而對發音品質做出更可靠的判斷。